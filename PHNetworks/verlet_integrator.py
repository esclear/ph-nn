# Optimizer for TensorFlow models utilizing Port-Hamiltonian systems
# Copyright (C) 2020  Daniel Albert
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import tensorflow as tf

class VerletIntegrator:
    """
        A verlet integrator written for Port-Hamiltonian systems used to train neural networks

        [N.B. the comments below are best read with a text editor that supports proper unicode symbols,
         including diacritics, such as codium or vs code]

        We assume that the Hamiltonian is of the form
            H(x) = L(Ï‘) + Â½Â·Ï‰áµ€Â·Mâ»Â¹Â·Ï‰.
        Where M = Máµ€ â‰» 0, x = (Ï‘,Ï‰) with Ï‰ = MÂ·Ï‘Ì‡.
        L is the loss function of the neuronal network, with respect to the networks' parameters Ï‘.

        This implies that
            âˆ‡Ï‘ H(x) = âˆ‡Ï‘ L(Ï‘),
            âˆ‡Ï‰ H(x) = Mâ»Â¹Â·Ï‰ = Ï‘Ì‡.

        The dynamics of the sytem are given by
            Ï‘Ì‡ =             âˆ‡Ï‰ H(x),
            Ï‰Ì‡ = -âˆ‡Ï‘ H(x) -BÂ·âˆ‡Ï‰ H(x)
        where B = Báµ€ â‰» 0.

        We can rewrite this as
            Ï‰Ì‡ = MÂ·Ï‘Ìˆ = -âˆ‡Ï‘ H(x) -BÂ·âˆ‡Ï‰ H(x)
        Thus, we can rewrite it as a second order differential equation:
            Ï‘Ìˆ = Mâ»Â¹Â·[- âˆ‡Ï‘ H(x) - BÂ·âˆ‡Ï‰ H(x)]
              = Mâ»Â¹Â·[- âˆ‡Ï‘ L(Ï‘) - BÂ·Mâ»Â¹Â·Ï‰]
              = Mâ»Â¹Â·[- âˆ‡Ï‘ L(Ï‘) - BÂ·Ï‘Ì‡]                        (S)

        We want to use the velocity verlet method for integration.
        This method is given by the steps
            Ï‘(t + Î”t) = Ï‘(t) + Î”tÂ·Ï‘Ì‡(t) + Â½Â·(Î”t)Â²Â·Ï‘Ìˆ(t)        (A)
            Ï‘Ì‡(t + Î”t) = Ï‘Ì‡(t) + Â½Â·Î”tÂ·[Ï‘Ìˆ(t) + Ï‘Ìˆ(t + Î”t)]       {B}

        However, in {B} Ï‘Ìˆ(t + Î”t) depends on Ï‘Ì‡(t + Î”t).
        But we can rectify this by solving for Ï‘Ìˆ(t + Î”t):
            Ï‘Ì‡(t + Î”t) = Ï‘Ì‡(t) + Â½Â·Î”tÂ·[Ï‘Ìˆ(t) + Ï‘Ìˆ(t + Î”t)] = Ï‘Ì‡(t) + Â½Â·Î”tÂ·[Ï‘Ìˆ(t) + Mâ»Â¹Â·[- âˆ‡Ï‘ L(Ï‘(t + Î”t)) - BÂ·Ï‘Ì‡(t + Î”t)]]
                      = Ï‘Ì‡(t) + Â½Â·Î”tÂ·[Ï‘Ìˆ(t) - Mâ»Â¹Â·âˆ‡Ï‘ L(Ï‘(t + Î”t)) - Mâ»Â¹Â·BÂ·Ï‘Ì‡(t + Î”t)]
          â‡” [Ï‘Ì‡(t + Î”t) + Â½Â·Î”tÂ·Mâ»Â¹Â·BÂ·Ï‘Ì‡(t + Î”t)] = Ï‘Ì‡(t) + Â½Â·Î”tÂ·[Ï‘Ìˆ(t) - Mâ»Â¹Â·âˆ‡Ï‘ L(Ï‘(t + Î”t))]
          â‡” [ğ•€ + Â½Â·Î”tÂ·Mâ»Â¹Â·B]Â·Ï‘Ì‡(t + Î”t) = Ï‘Ì‡(t) + Â½Â·Î”tÂ·[Ï‘Ìˆ(t) - Mâ»Â¹Â·âˆ‡Ï‘ L(Ï‘(t + Î”t))]
          â‡” Ï‘Ì‡(t + Î”t) = [ğ•€ + Â½Â·Î”tÂ·Mâ»Â¹Â·B]â»Â¹Â·[Ï‘Ì‡(t) + Â½Â·Î”tÂ·[Ï‘Ìˆ(t) - Mâ»Â¹Â·âˆ‡Ï‘ L(Ï‘(t + Î”t))]]     (B)

        Since both M and B are symmetric and positive definite, [ğ•€ + Â½Â·Î”tÂ·Mâ»Â¹Â·B]â»Â¹ exists.
        We let the user use arbitrary such matrices M and B, however in most cases these will be diagonal matrices,
        such that it is trivial to determine Mâ»Â¹, Bâ»Â¹ and even [ğ•€ + Â½Â·Î”tÂ·Mâ»Â¹Â·B]â»Â¹.

        This rather minimal implementation defers the calculation of the inverse matrices to the user, since they
        will have the best insights into the properties of M and B which may facilitate the computation of the inverse.
    """
    def __init__(self, loss_gradient, M, M_inv, B, B_inv, ITMB_inv):
        """
            Initialize the verlet integrator

            Parameters
            ----------

            loss_gradient : Function
                Function returning âˆ‡Ï‘ L(Ï‘) given parameters Ï‘

            M : tf.tensor
                The "mass" matrix M.
                This must be symmetric and positive definite (M = Máµ€ â‰» 0) by convention.

            M_inv : tf.tensor
                Mâ»Â¹, the inverse of M.

            B : tf.tensor
                The "resistive" or "dissipative" matrix B.
                This must be symmetric and positive definite (B = Báµ€ â‰» 0) by convention.

            B_inv : tf.tensor
                Bâ»Â¹, the inverse of B.

            ITMB_inv : Function
                Function which returns [ğ•€ + Â½Â·Î”tÂ·Mâ»Â¹Â·B]â»Â¹ when given Î”t
        """
        # Store provided parameters
        self.loss_gradient = loss_gradient

        self.M = M
        self.M_inv = M_inv

        self.B = B
        self.B_inv = B_inv

        self.ITMB_inv_fn = ITMB_inv

        # The dimension of the system, i.e. of the position / velocity
        self.dimension = M.shape[0]
        # Basic assertions to catch (some) errors
        assert self.M.shape == [self.dimension, self.dimension], "M has invalid dimensions"
        assert self.B.shape == [self.dimension, self.dimension], "The dimension of B does not match M"
        assert self.M_inv.shape == [self.dimension, self.dimension], "Dimensions imply that M_inv can't be the inverse of M"
        assert self.B_inv.shape == [self.dimension, self.dimension], "Dimensions imply that B_inv can't be the inverse of B"

        # Prepare position and velocity vector, where position corresponds to the network parameters and velocity to the impulse
        self.position         = tf.Variable(tf.zeros((self.dimension, 1), dtype='float64'))
        self.velocity         = tf.Variable(tf.zeros((self.dimension, 1), dtype='float64'))
        self.acceleration     = tf.Variable(tf.zeros((self.dimension, 1), dtype='float64'))

        # This technically does not /have/ to be a member of this class, however doing so we can avoid having compute the gradient
        # twice and to allocate memory and instantiate it in _velocity_verlet_step(), which is the function that will be called
        # most often.
        self.gradient         = tf.Variable(tf.zeros((self.dimension, 1), dtype='float64'))

        # Store the step size
        self.Î”t = tf.Variable(-1, dtype='float64')
        # We will store [ğ•€ + Â½Â·Î”tÂ·Mâ»Â¹Â·B]â»Â¹ here:
        self._itmb_inv = tf.sparse.eye(self.dimension, dtype='float64')


    def _update_stepsize(self, Î”t):
        # If the step size hasn't change, we don't need to do anything
        assert Î”t > 0, "Î”t must be positive"

        if self.Î”t == Î”t:
            return

        self.Î”t.assign(Î”t)
        self._itmb_inv = self.ITMB_inv_fn(Î”t)


    @tf.function
    def _velocity_verlet_step(self):
        """
            A step of the verlocity verlet method.
            We use the fomulas presented and derived above:

            Ï‘(t + Î”t) = Ï‘(t) + Î”tÂ·Ï‘Ì‡(t) + Â½Â·(Î”t)Â²Â·Ï‘Ìˆ(t)                                   (A)
            Ï‘Ì‡(t + Î”t) = [ğ•€ + Â½Â·Î”tÂ·Mâ»Â¹Â·B]â»Â¹Â·[Ï‘Ì‡(t) + Â½Â·Î”tÂ·[Ï‘Ìˆ(t) - Mâ»Â¹Â·âˆ‡Ï‘ L(Ï‘(t + Î”t))]]    (B)
            Ï‘Ìˆ(t + Î”t) = Mâ»Â¹Â·[- âˆ‡Ï‘ L(Ï‘(t + Î”t)) - BÂ·Ï‘Ì‡(t + Î”t)]
        """
        # Determine the new position via (A)
        self.position.assign(self.position + self.Î”t * self.velocity + 0.5 * tf.square(self.Î”t) * self.acceleration)

        # Determine the new velocity
        self.gradient.assign(self.loss_gradient(self.position))
        self.velocity.assign(
            tf.sparse.sparse_dense_matmul(
                self._itmb_inv,
                (
                    self.velocity
                    + 0.5 * self.Î”t * (
                        self.acceleration
                        - tf.sparse.sparse_dense_matmul(self.M_inv, self.gradient)
                    )
                )
            )
        )

        # Calculate the acceleration we would normally have used to calculate the velocity in (S),
        # since we use it as Ï‘Ìˆ(t) in (B) in the next iteration
        self.acceleration.assign(
            tf.sparse.sparse_dense_matmul(self.M_inv, (- self.gradient - tf.sparse.sparse_dense_matmul(self.B, self.velocity)))
        )


    def integrate(self, t, Î”t, initial_position=None, initial_velocity=None):
        """
            Integrate the system over the interval [0, t], taking steps of size Î”t.

            For simplicity, the implementation is such that if t is not a multiple of Î”t, this implementation will
            actually integrate over the interval [0, k * Î”t], such that k is minimal with k * Î”t â‰¥ t.

            Parameters
            ----------
            t : float
                The duration over which the differential equation shall be integrated.
            Î”t : float
                Step size to use for the integration
            initial_position : tf.tensor
                The initial position (parameters) of the neural network.
                Uses the last known parameters if not specified.
            initial_velocity : tf.tensor
                The initial velocity (given by the impulse) of the state.
                Uses the last known velocity if not specified.
        """
        self._update_stepsize(Î”t)

        if initial_position is not None:
            self.position.assign(initial_position)
        if initial_velocity is not None:
            self.velocity.assign(initial_velocity)

        s = tf.Variable(0, dtype='float64')
        while s < t:
            self._velocity_verlet_step()
            s.assign(s + Î”t)

        return self.position, self.velocity